{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\chide\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chide\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chide\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chide\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import EnglishStemmer  # Assuming we're working with English\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    \n",
    "    \n",
    "    def manchesterUni_crawler(self, url, data):\n",
    "    \n",
    "        html_ = urlopen(url)\n",
    "        soup = BeautifulSoup(html_, 'lxml')\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            i = a_tag.get('href')\n",
    "            li = a_tag.find_parent('li')\n",
    "            if type(i) == str and i.find('https://www.research.manchester.ac.uk/portal/') != -1:\n",
    "                links.append(i)\n",
    "        for n in links:\n",
    "            html = urlopen(n)\n",
    "            soup2 = BeautifulSoup(html, 'lxml')\n",
    "            for re_link in soup2.find_all('a'):\n",
    "                atag = re_link.get('href')\n",
    "                if type(atag) == str and atag.find('/projects.html?period=running') != -1:\n",
    "                    html2 = urlopen(atag)\n",
    "                    soup3 = BeautifulSoup(html2, 'lxml')\n",
    "                    divv = soup3.find_all('div', class_ = 'profileinformation')\n",
    "                    for p in divv:\n",
    "                        name = soup2.find(class_ = 'person')\n",
    "                        names = name.text\n",
    "                        if names and p.text:\n",
    "                            data.append([names, n , p.text])\n",
    "    \n",
    "    def newcastleUni_crawler(self, url, data):  \n",
    "        html_ = urlopen(url)\n",
    "        soup = BeautifulSoup(html_, 'lxml')\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            i = a_tag.get('href')\n",
    "            li = a_tag.find_parent('li')\n",
    "            if type(i) == str and i.find('profile/') != -1:\n",
    "                links.append('https://www.ncl.ac.uk/'+i)\n",
    "        for n in links:\n",
    "                html = urlopen(n)\n",
    "                soup2 = BeautifulSoup(html, 'lxml')\n",
    "                main = soup2.find(id = 'content')\n",
    "                article = main.find('article')\n",
    "                h1 = article.find_all('h1', class_=\"\")\n",
    "                parent_div = article.find('div', class_='contentSeparator tab')\n",
    "                for h2 in article.find_all('h2'):\n",
    "                    if h2.text == 'Research' and h1[0] != '':\n",
    "                        div_res = h2.find_parent('div')\n",
    "                        data.append([h1[0].text, n, div_res.text])\n",
    "    \n",
    "    def birminghamUni_crawler(self, url, data):\n",
    "        html_ = urlopen(url)\n",
    "        soup = BeautifulSoup(html_, 'lxml')\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            i = a_tag.get('href')\n",
    "            if i is not None and type(i) == str and (i.find('/schools/gees/people/profile.aspx') != -1 or i.find('staff/profiles/eese/') != -1):\n",
    "                links.append('https://www.birmingham.ac.uk'+i)\n",
    "        for n in links:\n",
    "            yourstring = n.encode('ascii', 'ignore').decode('ascii')\n",
    "            html = urlopen(yourstring)\n",
    "            soup2 = BeautifulSoup(html, 'lxml')\n",
    "            main = soup2.find(id='maincontent')\n",
    "            article = main.find('article')\n",
    "            name = article.find('h1', class_='')\n",
    "            staff = article.find(id='staffdetails')\n",
    "            if staff is not None:\n",
    "                for section in staff.find_all('section'):\n",
    "                    if section is not None:  \n",
    "                        h2 = section.find('h2')\n",
    "                        if h2 is not None and h2.text.find('Research') != -1:\n",
    "                            res = section.text\n",
    "\n",
    "            for span in name.find_all('span'):\n",
    "                if span.text in name.text:\n",
    "                    names = name.text.replace(span.text, '')\n",
    "                    data.append([names, n, res])\n",
    "    \n",
    "    def birmingham2(self, data):\n",
    "    \n",
    "        html_ = urlopen('https://www.cs.bham.ac.uk/people/')\n",
    "        soup = BeautifulSoup(html_, 'lxml')\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            i = a_tag.get('href')\n",
    "            if i is not None and type(i) == str and i.find('staff/profiles/computer-science/') != -1:\n",
    "                 links.append(i)\n",
    "        for m in links:\n",
    "            html__ = urlopen(m)\n",
    "            soup3 = BeautifulSoup(html__, 'lxml')\n",
    "            main1 = soup3.find(id='maincontent')\n",
    "            article1 = main1.find('article')\n",
    "            name = article1.find('h1', class_='')\n",
    "            staff1 = article1.find(id='staffdetails')\n",
    "            if staff1 is not None:\n",
    "                for sections in staff1.find_all('section'):\n",
    "                    h2 = sections.find('h2')\n",
    "                    if h2 is not None and h2.text.find('Research') != -1:\n",
    "                        rese = sections.text\n",
    "                        for span in name.find_all('span'):\n",
    "                            if span.text in name.text:\n",
    "                                prof_name = name.text.replace(span.text, '')\n",
    "                                data.append([prof_name, m, rese])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler()\n",
    "\n",
    "urls = ['https://www.ees.manchester.ac.uk/about/people/academic-and-research-staff/',\n",
    "        'https://www.mace.manchester.ac.uk/about/people/academic-and-research-staff/', \n",
    "        'https://www.cs.manchester.ac.uk/about/people/academic-and-research-staff/'\n",
    "       ]  # urls to crawl from Manchester Uni\n",
    "\n",
    "urls1 = ['https://www.ncl.ac.uk/engineering/research/civil/people/#researchstaff',\n",
    "        'https://www.ncl.ac.uk/computing/people/research/', \n",
    "        'https://www.ncl.ac.uk/nes/staff/research/'\n",
    "       ]  # urls to crawl from Newcastle Uni\n",
    "\n",
    "urls2 = ['https://www.birmingham.ac.uk/schools/engineering/about/academic.aspx',\n",
    "        'https://www.birmingham.ac.uk/schools/gees/people/index.aspx', \n",
    "       ]  # urls to crawl from Birmingham Uni\n",
    "\n",
    "dt1 = [] # es\n",
    "dt2 = [] # ce\n",
    "dt3 = [] # cs\n",
    "\n",
    "dt4 = [] #ce\n",
    "dt5 = [] #cs\n",
    "dt6 = [] #es\n",
    "\n",
    "dt7 = [] #ce\n",
    "dt8 = [] #es\n",
    "dt9 = [] #cs\n",
    "\n",
    "crawler.manchesterUni_crawler(urls[0], dt1)\n",
    "crawler.manchesterUni_crawler(urls[1], dt2)\n",
    "crawler.manchesterUni_crawler(urls[2], dt3)\n",
    "\n",
    "crawler.newcastleUni_crawler(urls1[0], dt4)\n",
    "crawler.newcastleUni_crawler(urls1[1], dt5)\n",
    "crawler.newcastleUni_crawler(urls1[2], dt6)\n",
    "\n",
    "crawler.birminghamUni_crawler(urls2[0], dt7)\n",
    "crawler.birminghamUni_crawler(urls2[1], dt8)\n",
    "crawler.birmingham2(dt9)\n",
    "\n",
    "df1 = pd.DataFrame(dt1, columns=['name', 'link', 'research'])\n",
    "df2 = pd.DataFrame(dt2, columns=['name', 'link', 'research'])\n",
    "df3 = pd.DataFrame(dt3, columns=['name', 'link', 'research'])\n",
    "\n",
    "df4 = pd.DataFrame(dt4, columns=['name', 'link', 'research'])\n",
    "df5 = pd.DataFrame(dt5, columns=['name', 'link', 'research'])\n",
    "df6 = pd.DataFrame(dt6, columns=['name', 'link', 'research'])\n",
    "\n",
    "df7 = pd.DataFrame(dt7, columns=['name', 'link', 'research'])\n",
    "df8 = pd.DataFrame(dt8, columns=['name', 'link', 'research'])\n",
    "df9 = pd.DataFrame(dt9, columns=['name', 'link', 'research'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_dept = pd.concat([df3, df5, df9], ignore_index=True)\n",
    "ce_dept = pd.concat([df2, df4, df7], ignore_index=True)\n",
    "es_dept = pd.concat([df1, df6, df8], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addField(field_data, field = ''):\n",
    "    data_dept = []\n",
    "    \n",
    "    for x in range(len(field_data['name'])):\n",
    "        x = field\n",
    "        data_dept.append(x)\n",
    "    field_data['field'] = data_dept\n",
    "    return field_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "      <th>research</th>\n",
       "      <th>field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Professor Grant Allen</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/G...</td>\n",
       "      <td>\\nResearch interestsGrant is currently active ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Professor Grant Allen</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/G...</td>\n",
       "      <td>\\nOther researchDoctoral Thesis\\n\\nGrant's PhD...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prof Richard Bardgett</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/r...</td>\n",
       "      <td>\\nResearch interestsMy research is broadly con...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr Stephen Boult</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/s...</td>\n",
       "      <td>\\nResearch interestsHydrology and hydrochemist...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prof Terence Brown BSc, PhD, FSA</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/e...</td>\n",
       "      <td>\\nResearch interestsBiomolecular Archaeology\\n...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Dr Martin Widmann</td>\n",
       "      <td>https://www.birmingham.ac.uk/schools/gees/peop...</td>\n",
       "      <td>\\n\\n\\r\\n\\t\\t\\t\\t\\t\\t\\tResearch\\n\\n\\nCurrent re...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Dr Jingsha Xu</td>\n",
       "      <td>https://www.birmingham.ac.uk/schools/gees/peop...</td>\n",
       "      <td>\\n\\n\\r\\n\\t\\t\\t\\t\\t\\t\\tResearch\\n\\n\\nCurrent re...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Dr Peng Zhang</td>\n",
       "      <td>https://www.birmingham.ac.uk/schools/gees/peop...</td>\n",
       "      <td>\\n\\n\\r\\n\\t\\t\\t\\t\\t\\t\\tResearch\\n\\n\\nEnvironmen...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Dr Zhiling Zhiling Guo</td>\n",
       "      <td>https://www.birmingham.ac.uk/schools/gees/peop...</td>\n",
       "      <td>\\n\\n\\r\\n\\t\\t\\t\\t\\t\\t\\tResearch\\n\\n\\nResearch I...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Dr Jian Zhong</td>\n",
       "      <td>https://www.birmingham.ac.uk/schools/gees/peop...</td>\n",
       "      <td>\\n\\n\\r\\n\\t\\t\\t\\t\\t\\t\\tResearch\\n\\n\\nResearch I...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name  \\\n",
       "0               Professor Grant Allen   \n",
       "1               Professor Grant Allen   \n",
       "2               Prof Richard Bardgett   \n",
       "3                    Dr Stephen Boult   \n",
       "4    Prof Terence Brown BSc, PhD, FSA   \n",
       "..                                ...   \n",
       "184               Dr Martin Widmann     \n",
       "185                   Dr Jingsha Xu     \n",
       "186                   Dr Peng Zhang     \n",
       "187          Dr Zhiling Zhiling Guo     \n",
       "188                   Dr Jian Zhong     \n",
       "\n",
       "                                                  link  \\\n",
       "0    https://www.research.manchester.ac.uk/portal/G...   \n",
       "1    https://www.research.manchester.ac.uk/portal/G...   \n",
       "2    https://www.research.manchester.ac.uk/portal/r...   \n",
       "3    https://www.research.manchester.ac.uk/portal/s...   \n",
       "4    https://www.research.manchester.ac.uk/portal/e...   \n",
       "..                                                 ...   \n",
       "184  https://www.birmingham.ac.uk/schools/gees/peop...   \n",
       "185  https://www.birmingham.ac.uk/schools/gees/peop...   \n",
       "186  https://www.birmingham.ac.uk/schools/gees/peop...   \n",
       "187  https://www.birmingham.ac.uk/schools/gees/peop...   \n",
       "188  https://www.birmingham.ac.uk/schools/gees/peop...   \n",
       "\n",
       "                                              research field  \n",
       "0    \\nResearch interestsGrant is currently active ...    es  \n",
       "1    \\nOther researchDoctoral Thesis\\n\\nGrant's PhD...    es  \n",
       "2    \\nResearch interestsMy research is broadly con...    es  \n",
       "3    \\nResearch interestsHydrology and hydrochemist...    es  \n",
       "4    \\nResearch interestsBiomolecular Archaeology\\n...    es  \n",
       "..                                                 ...   ...  \n",
       "184  \\n\\n\\r\\n\\t\\t\\t\\t\\t\\t\\tResearch\\n\\n\\nCurrent re...    es  \n",
       "185  \\n\\n\\r\\n\\t\\t\\t\\t\\t\\t\\tResearch\\n\\n\\nCurrent re...    es  \n",
       "186  \\n\\n\\r\\n\\t\\t\\t\\t\\t\\t\\tResearch\\n\\n\\nEnvironmen...    es  \n",
       "187  \\n\\n\\r\\n\\t\\t\\t\\t\\t\\t\\tResearch\\n\\n\\nResearch I...    es  \n",
       "188  \\n\\n\\r\\n\\t\\t\\t\\t\\t\\t\\tResearch\\n\\n\\nResearch I...    es  \n",
       "\n",
       "[189 rows x 4 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addField(cs_dept, 'cs')\n",
    "addField(ce_dept, 'ce')\n",
    "addField(es_dept, 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "64\n",
      "189\n",
      "550\n"
     ]
    }
   ],
   "source": [
    "bigData2 = pd.concat([cs_dept, ce_dept, es_dept], ignore_index=True)\n",
    "print(len(ce_dept))\n",
    "print(len(cs_dept))\n",
    "print(len(es_dept))\n",
    "print(len(bigData2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigData2 = bigData2.assign(id=(bigData2['name'].astype('category').cat.codes))\n",
    "bigData2.to_csv('crawled_uni.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "      <th>research</th>\n",
       "      <th>field</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Prof Sophia Ananiadou</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/s...</td>\n",
       "      <td>\\nResearch interestsProf. Ananiadou's main con...</td>\n",
       "      <td>cs</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Dr Richard Banach</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/r...</td>\n",
       "      <td>\\nResearch interestsFor accurate and up to dat...</td>\n",
       "      <td>cs</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Prof Gavin Brown</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/g...</td>\n",
       "      <td>\\nResearch interestsMachine Learning.\\n</td>\n",
       "      <td>cs</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Dr Ke Chen</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/k...</td>\n",
       "      <td>\\nResearch interestsMy general areas of intere...</td>\n",
       "      <td>cs</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Prof Timothy Cootes</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/t...</td>\n",
       "      <td>\\nResearch interestsMedical Image Analysis usi...</td>\n",
       "      <td>cs</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Prof Timothy Cootes</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/t...</td>\n",
       "      <td>\\nMethodological knowledge\\r\\n\\tComputer Visio...</td>\n",
       "      <td>cs</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Dr Suzanne Embury</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/s...</td>\n",
       "      <td>\\nOther research\\nProjects\\n\\n\\nInstitute of C...</td>\n",
       "      <td>cs</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Prof Steve Furber CBE FRS FREng DFBCS FIET CIT...</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/s...</td>\n",
       "      <td>\\nResearch interestsAPT Projects\\n</td>\n",
       "      <td>cs</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Prof Carole Goble</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/c...</td>\n",
       "      <td>\\nResearch interestsCarole Goble’s research is...</td>\n",
       "      <td>cs</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Prof Anthony Goodacre</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/j...</td>\n",
       "      <td>\\nResearch interestsHaving defined and introdu...</td>\n",
       "      <td>cs</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               name  \\\n",
       "0           0                              Prof Sophia Ananiadou   \n",
       "1           1                                  Dr Richard Banach   \n",
       "2           2                                   Prof Gavin Brown   \n",
       "3           3                                         Dr Ke Chen   \n",
       "4           4                                Prof Timothy Cootes   \n",
       "5           5                                Prof Timothy Cootes   \n",
       "6           6                                  Dr Suzanne Embury   \n",
       "7           7  Prof Steve Furber CBE FRS FREng DFBCS FIET CIT...   \n",
       "8           8                                  Prof Carole Goble   \n",
       "9           9                              Prof Anthony Goodacre   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.research.manchester.ac.uk/portal/s...   \n",
       "1  https://www.research.manchester.ac.uk/portal/r...   \n",
       "2  https://www.research.manchester.ac.uk/portal/g...   \n",
       "3  https://www.research.manchester.ac.uk/portal/k...   \n",
       "4  https://www.research.manchester.ac.uk/portal/t...   \n",
       "5  https://www.research.manchester.ac.uk/portal/t...   \n",
       "6  https://www.research.manchester.ac.uk/portal/s...   \n",
       "7  https://www.research.manchester.ac.uk/portal/s...   \n",
       "8  https://www.research.manchester.ac.uk/portal/c...   \n",
       "9  https://www.research.manchester.ac.uk/portal/j...   \n",
       "\n",
       "                                            research field   id  \n",
       "0  \\nResearch interestsProf. Ananiadou's main con...    cs  413  \n",
       "1  \\nResearch interestsFor accurate and up to dat...    cs  267  \n",
       "2            \\nResearch interestsMachine Learning.\\n    cs  375  \n",
       "3  \\nResearch interestsMy general areas of intere...    cs  170  \n",
       "4  \\nResearch interestsMedical Image Analysis usi...    cs  419  \n",
       "5  \\nMethodological knowledge\\r\\n\\tComputer Visio...    cs  419  \n",
       "6  \\nOther research\\nProjects\\n\\n\\nInstitute of C...    cs  314  \n",
       "7                 \\nResearch interestsAPT Projects\\n    cs  414  \n",
       "8  \\nResearch interestsCarole Goble’s research is...    cs  365  \n",
       "9  \\nResearch interestsHaving defined and introdu...    cs  359  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df = pd.read_csv('crawled_uni.csv')\n",
    "big_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                  190\n",
       "name                          Dr Maria Sharmina PhD, AMEI, FHEA\n",
       "link          https://www.research.manchester.ac.uk/portal/m...\n",
       "research      \\nResearch interestsEnergy systemsFood-energy-...\n",
       "field                                                        ce\n",
       "id                                                          200\n",
       "Name: 190, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry = big_df.loc[190,:].copy()\n",
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Build_Indexer:\n",
    "    \n",
    "    def __init__(self, posTag, stopwords, lemmatize):\n",
    "        self.posTag = posTag\n",
    "        self.stopwords = stopwords\n",
    "        self.lem = lemmatize\n",
    "    \n",
    "    \n",
    "    def process_word(self, word):\n",
    "        word = word.lower() #to lowercase\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation)) #strip punctuation\n",
    "        word = self.lemma_stop(word)\n",
    "        return word\n",
    "        \n",
    "    def get_word_pos(self, word):\n",
    "        tag = self.posTag([word])[0][1][0].upper() # convert word uppercase\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV} #identify word meaning\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "        \n",
    "    def lemma_stop(self, doc):\n",
    "        stop = self.stopwords.words('english') # get english stopwords from lib \n",
    "\n",
    "        tokens = nltk.word_tokenize(doc) # tokenize the word\n",
    "        string = \"\"\n",
    "        for n in tokens: \n",
    "            if n not in stop: #check if word is a stopword\n",
    "                string += self.lem.lemmatize(n, self.get_word_pos(n)) + \" \" #lemmatize word\n",
    "        return string \n",
    "    \n",
    "    def transform_data(self, df):\n",
    "        df = df\n",
    "        df['name'] = df['name'].apply(self.process_word) # process each name in document\n",
    "        df['research'] = df['research'].apply(self.process_word) #process each research entry\n",
    "        df['text'] = df['name'] + \" \" + df['research'] \n",
    "        drop_cols = ['name', 'link', 'research']\n",
    "        df = df.drop(drop_cols, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def index_entry(self, entry, index):\n",
    "        words = entry.text.split() #split the words in an entry\n",
    "        Id = entry['id']\n",
    "        \n",
    "        for word in words:\n",
    "            if word in index.keys(): # check if word is in keys\n",
    "                  if Id not in index[word]:\n",
    "                        index[word].append(Id) # add word to specific id\n",
    "            else:\n",
    "                index[word] = [Id]\n",
    "        return index\n",
    "    \n",
    "    def index_data(self, df, index): # get index of the whole data\n",
    "        for i in range(len(df)):\n",
    "            entry = df.loc[i,:]\n",
    "            index = self.index_entry(entry = entry, index = index) #index each entry\n",
    "        return index\n",
    "    \n",
    "    def index_build(self, df, index): #buid the final inverted index of the data\n",
    "        add = self.transform_data(df)\n",
    "        index = self.index_data(df = add, index = index)\n",
    "        return index\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dr maria sharmina phd amei fhea '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "indexer = Build_Indexer(pos_tag, stopwords, lem)\n",
    "\n",
    "indexer.process_word(entry['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jud hghk ugjhl '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time indexer.process_word('JUD, HGHK,  UGJHL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_processed = big_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_processed['text'] = big_processed['name'] + \" \" + big_processed['research']\n",
    "drop_cols = ['name', 'link', 'research', 'field']\n",
    "big_processed = big_processed.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dr': [170], 'Ke': [170], 'Chen': [170], 'Research': [170], 'interestsMy': [170], 'general': [170], 'areas': [170], 'of': [170], 'interest': [170], 'are': [170], 'artificial': [170], 'intelligence': [170], 'and': [170], 'computational': [170], 'cognitive': [170], 'science.': [170], 'In': [170], 'particular,': [170], 'I': [170], 'have': [170], 'been': [170], 'working': [170], 'in': [170], 'machine': [170], 'learning,': [170], 'pattern': [170], 'recognition,': [170], 'perception,': [170], 'systems': [170], 'their': [170], 'applications': [170], 'intelligent': [170], 'system': [170], 'development': [170], 'for': [170], 'over': [170], 'three': [170], 'decades.': [170], 'With': [170], 'the': [170], 'aforementioned': [170], 'fundamental': [170], 'researches,': [170], 'am': [170], 'keen': [170], 'on': [170], 'developing': [170], 'effective': [170], 'efficient': [170], 'techniques': [170], 'real': [170], 'world': [170], 'applications.': [170], 'For': [170], 'my': [170], 'current': [170], 'research': [170], 'topics,': [170], 'please': [170], 'visit': [170], 'Interest': [170], 'page.': [170], 'further': [170], 'information': [170], 'interests,': [170], 'Publication': [170]}\n"
     ]
    }
   ],
   "source": [
    "entry = big_processed.loc[3,:].copy()\n",
    "\n",
    "ind = indexer.index_entry(entry=entry, index= {})\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23540"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_all = indexer.index_data(big_processed, index = {})\n",
    "len(ind_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = indexer.index_build(df= big_df, index = {})\n",
    "#print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input('Enter search term: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/chide/OneDrive/Desktop/Info Retrieval/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query_Index:\n",
    "    \n",
    "    def __init__(self, wordVec, indexer, bigD, index):\n",
    "        self.wordVec = wordVec\n",
    "        self.indexer = indexer\n",
    "        self.bigD = bigD\n",
    "        self.index = index\n",
    "  \n",
    "    def process_query(self, query):\n",
    "        normalize = indexer.process_word(query)\n",
    "        return normalize.split()\n",
    "    \n",
    "    def get_vec_av(self, wordVec, doc):\n",
    "        wordVec = self.wordVec\n",
    "        doc = [word for word in doc if word in wordVec.vocab]\n",
    "        if len(doc) == 0:\n",
    "            return np.zeros(300)\n",
    "        else:\n",
    "            return np.mean(wordVec[doc], axis=0) \n",
    "        \n",
    "    def prepare_for_ranking(self, df):\n",
    "        corpus = df[['id', 'research']].copy()\n",
    "        doc_vecs = {}\n",
    "        for i in range(len(corpus)):\n",
    "            row = corpus.loc[i,:]\n",
    "            text = row.research.split()\n",
    "        doc_vecs[row.id]= self.get_vec_av(self.wordVec, text)\n",
    "        doc_vecs = pd.DataFrame.from_dict(data=doc_vecs, orient=\"index\")\n",
    "        doc_vecs['id'] = doc_vecs.index\n",
    "        return doc_vecs\n",
    "    \n",
    "    \n",
    "    def lists_intersection(self, lists):\n",
    "        intersect = list(set.intersection(*map(set, lists)))\n",
    "        intersect.sort()\n",
    "        return intersect\n",
    "    \n",
    "    def search_en(self, query, index):\n",
    "        index = self.index\n",
    "        query_split = self.process_query(query)\n",
    "        retrieved = []\n",
    "        for word in query_split:\n",
    "            if word in index.keys():\n",
    "                retrieved.append(index[word])\n",
    "            if len(retrieved)>0:\n",
    "                result = self.lists_intersection(retrieved)\n",
    "            else:\n",
    "                result = [0]\n",
    "        return result\n",
    "    \n",
    "    def connect_id(self, retrieved_ids, df):\n",
    "        \n",
    "        return df[df['id'].isin(retrieved_ids)].reset_index(drop=True)\n",
    "    \n",
    "    def search_query(self, query):\n",
    "        result_id = self.search_en(query, self.bigD)\n",
    "        results = self.connect_id(result_id, self.bigD)\n",
    "        results = self.rank_results(query, results)\n",
    "        self.print_results(results)\n",
    "            \n",
    "    def cos_similarity(self, a, b):\n",
    "        dot = np.dot(a, b)\n",
    "        norma = np.linalg.norm(a)\n",
    "        normb = np.linalg.norm(b)\n",
    "        cos = dot / (norma * normb)\n",
    "        return(cos)\n",
    "    \n",
    "    def rank_results(self, query, results):\n",
    "        query_norm = self.process_query(query)\n",
    "        query_vec = self.get_vec_av(self.wordVec, query_norm)\n",
    "        res_vecs = self.connect_id(results['id'], self.prepare_for_ranking(self.bigD))\n",
    "        cos_array = []\n",
    "        for i in range(len(res_vecs)):\n",
    "            doc_vec = res_vecs.loc[i,:].drop(['id'])\n",
    "            cos_array.append(self.cos_similarity(doc_vec, query_vec))\n",
    "            results['rank'] = cos_array\n",
    "            results = results.sort_values('rank', axis=0, ascending=True)\n",
    "        return results\n",
    "    \n",
    "    def print_results(self, result_df):\n",
    "        for i in range(len(result_df)):\n",
    "            res = result_df.loc[i, :]\n",
    "            print(res['name'])\n",
    "            print(res['research'])\n",
    "            print(res['field'])\n",
    "            if i == len(result_df):\n",
    "                print(res.link)\n",
    "            else:\n",
    "                print(\"{}\\n\" .format(res.link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_D = big_df.drop(['text'], axis=1).copy()\n",
    "\n",
    "idx = indexer.index_build(df= big_df, index = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "query_in = Query_Index(word2vec, indexer, big_D, idx)\n",
    "\n",
    "words = entry.text.split()\n",
    "\n",
    "%time test_vec = query_in.get_vec_av(word2vec, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs = query_in.prepare_for_ranking(df=big_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: Computer.\n",
      "Normalized query: computer .\n"
     ]
    }
   ],
   "source": [
    "test = \"Computer\"\n",
    "print(\"User query: {}.\" .format(test))\n",
    "test_norm = indexer.process_word(test)\n",
    "print(\"Normalized query: {}.\" .format(test_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 14, 106, 112, 117, 143, 186, 210, 238, 239, 318, 336, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 431, 443, 448, 455, 459, 461]\n"
     ]
    }
   ],
   "source": [
    "results = query_in.search_en(\"prof\", idx)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Search for: \") # search for query\n",
    "query_in.search_query(query) # display according to rank \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "tfidfconv = TfidfTransformer()\n",
    "big_X = big_df['research']\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "\n",
    "X = vectorizer.fit_transform(big_X).toarray() \n",
    "y = big_df['field'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, random_state=0)  \n",
    "clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48  2  8]\n",
      " [ 3  4  1]\n",
      " [ 6  1 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ce       0.84      0.83      0.83        58\n",
      "          cs       0.57      0.50      0.53         8\n",
      "          es       0.80      0.84      0.82        44\n",
      "\n",
      "    accuracy                           0.81       110\n",
      "   macro avg       0.74      0.72      0.73       110\n",
      "weighted avg       0.81      0.81      0.81       110\n",
      "\n",
      "0.8090909090909091\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
